6. Wyszukiwanie pełnotekstowe w archiwum cyfrowym i text mining

Tradycyjne archiwum udostępniające oryginalne dokumenty lub ich analogowe reprodukcje, np. w postaci mikrofilmów, przerzuca na badacza całą pracę zwią- zaną z analizowaniem ich treści. Archiwiści przygotowują co prawda podsta- wowy opis na poziomie zestawu metadanych, jednak odczytanie treści doku- mentu w poszukiwaniu konkretnych informacji jest już zadaniem osoby korzy- stającej ze zbioru. Jak odbywa się to w sytuacji, kiedy archiwalia udostępniane są w Internecie w formie pełnotekstowej, tzn. pozwalającej na automatyczne prze- szukiwanie ich treści pod kątem wybranych słów?
Przykładem takiego archiwum jest brytyjski projekt London Lives (http://www.londonlives.org/), udostępniający źródła do historii kryminalnej dawnego Londynu. Każdy dokument publikowany jest w dwóch wersjach: stan- dardowego skanu (pliku graficznego) i w formie tekstowej, opartej na odpowied- nim schemacie XML. Nie jest to więc zwykła transkrypcja treści, polegająca na udostępnieniu jednego ciągu tekstu, ale ustandaryzowany zestaw pól seman- tycznych. Każdy element treści, np. podpis sędziego, zapis informacyjny o dacie rozpatrywania sprawy karnej, imię i nazwisko osoby sądzonej, skreślenie itp., opisany jest odpowiednim znacznikiem (tagiem), pozwalającym systemowi roz- poznać znaczenie treści przetwarzanych dokumentów:


<rs type="persName" id="OA16901024_n12-1">Peter Vallard</rs>
<interp inst="OA16901024_n12-1" type="given" value="PETER"></interp> <interp inst="OA16901024_n12-1" ty-pe="surname" value="VALLARD"></interp> <interp
in-st="OA16901024_n12-1"type="gender"value="male"></interp> […] He was a working <rs type="occupation" id="OA16901024_occ2">Silver-Smith</rs>


Powyższy fragment pochodzi z wydanej drukiem w 1690 roku publikacji opraco- wanej przez więzienie w Newgate, w której zamieszczano krótkie biografie osób skazanych na śmierć oraz przytaczano ich ostatnie słowa przed wykonaniem eg- zekucji. Informacja o Peterze Vallardzie, Francuzie skazanym za zdradę stanu, została uzupełniona wpisem o jego zawodzie (był złotnikiem, ang. silver-smith). Treść tego fragmentu opisana jest za pomocą odpowiednich tagów, pozwala- jących  tworzyć  specjalistyczne  indeksy czy przeszukiwać  treść  dokumentów udostępnianych w serwisie pod kątem płci, imion i nazwisk skazanych czy za- wodów przez nich wykonywanych.  XML określa sposób, w jaki zapisywane są dane wraz z ich strukturą, pozwalając oprogramowaniu rozpoznawać znacze- nie zgromadzonych informacji. Spróbujmy zanalizować jeszcze jeden przykład, tym razem fikcyjnej bazy biogramów ofiar cywilnych powstania warszawskiego. Fragment przygotowanego przez historyka rekordu w postaci zwykłego tekstu:

5 sierpnia 1944 Jan i jego matka zostali wraz z innymi mieszkańcami Woli wypędzeni z domów w okolice torów kolejowych przy ulicy Gór- czewskiej.

po uzupełnieniu odpowiednio zaprojektowanymi znacznikami XML uzyskać może następującą postać:

<data>5 sierpnia 1944</data> <osoba id="Jan Adamczewski"><imie>Jan</imie></osoba> i jego <osoba id="Teresa Adamczewska">matka</a> zostali wraz z innymi mieszkańcami
<miejscetyp="dzielnica">Woli</miejsce> wypędzeni z domów w okolice torówkolejowych przy ulicy <miejsce typ="ulica">Górczewskiej</miejsce>.

Dzięki takiemu zapisowi użytkownik bazy biogramów mógłby przeszukiwać ją pod kątem imion i nazwisk wspomnianych w nich osób czy robić kwerendę szu- kając konkretnych nazw dzielnic lub ulic czy dat. W przypadku wyszukiwania pełnotekstowego w pozbawionej znaczników XML treści, wyszukiwarka nie mogłaby rozpoznać, że wspomniany w relacji Jan to Jan Adamczewski. Praca po- legająca na sematycznym opisaniu narracji pozwoli użytkownikowi skuteczniej badać udostępniane w bazie materiały. Przygotowanie takiej formy udostępniania treści historycznych jest bardzo pracochłonne, ponieważ poza standardową trans- krypcją należy opisać treść dokumentu za pomocą semantycznych znaczników i trzymać się przy tym przyjętego standardu opisu.
Zdaniem Tima Hitchcocka, twórcy archiwum London Lives i kilku podob- nych projektów, dostępność przeszukiwanych pełnotekstowo wersji dokumentów historycznych prowadzi do redukowania roli archiwum i znaczenia struktury, w ramach której udostępniane są dokumenty (zespoły i jednostki archiwalne). Do informacji źródłowych interesujących badacza dotrzeć można przecież bezpo- średnio, po wpisaniu odpowiednich fraz w wyszukiwarkę, przeszukującą wszyst- kie treści ponad strukturą zbiorów. Dzięki digitalizacji uwalniamy się zatem w pewnym sensie spod władzy organizacji archiwum (Hitchcock, 2008, s. 89). Efektem ubocznym pełnotekstowego przeszukiwania zdigitalizowanych zbiorów może być jednak utrata kontekstu, nadającego sens wyszukiwanym informacjom:

[...] przeszukiwanie po słowach kluczowych (keyword searching) tek- stów drukowanych i – podobnie – kolekcji wizualnych dramatycznie wpłynęło na to, jak prowadzimy badania i czego szukamy. [...] Wy- bieramy frazę wyszukiwania lub zestaw takich fraz i przeszukujemy bezkrytycznie literaturę, dowody zbrodni, elektroniczne katalogi i ga- zety. W tym procesie często zanika wymóg (albo choćby możliwość) zrozumienia kontekstu, w ramach którego ujawnia się nam wybrana in- formacja (Hitchcock 2008, s. 85).

Hitchcock zwraca także uwagę na to, że dzięki digitalizacji i udostępnianiu miliardów słów z dawnych publikacji historycy coraz chętniej obudowują przy- gotowywane przez siebie przekazy o faktach za pomocą źródeł narracyjnych.
Pójdźmy jednak krok dalej. Skoro archiwum cyfrowe może udostępniać dokumenty historyczne w formie ustandaryzowanych plików XML, z seman- tycznym opisem poszczególnych fragmentów ich treści, do przeszukiwania tych dokumentów może zostać wykorzystany odpowiedni, automatycznie działający algorytm. W archiwum cyfrowym Old Bailey Online (http://www.oldbaileyonline.org), podobnym do projektu London Lives, można analizować automatycznie 127 milionów słów, zapisanych w dokumentach ze śledztw i procesów z lat 1674–1913, z wykorzystaniem API. Dzięki zastosowa- niu odpowiednich skryptów, korzystających z danych udostępnianych przez Old Bailey Online, praca historyka ulega usprawnieniu, ponieważ w krótkim czasie może on zanalizować nawet obszerne zbiory. Oczywiście w tym miejscu pojawia się pytanie o wartości poznawcze takiego automatycznego badania, w odniesie- niu do tradycyjnego, manualnego przeglądania dokumentów, jeden po drugim. Przygotowanie algorytmu ściągającego odpowiednie informacje z bazy danych archiwum wymaga odpowiednich kompetencji programistycznych, co skłania z kolei do postawienia pytania o edukację informatyczną historyków. Ci ostatni mogą przecież nie tylko korzystać z tego typu archiwów, ale też je tworzyć, np. udostępniając za ich pomocą zbiory wykorzystywane do własnych badań. Problem kompetencji wiąże się tutaj także z problemem współpracy między hi- storykiem, archiwistą i informatykiem: czy studia historyczne przygotowują do pracy grupowej, uczą zarządzania projektami badawczymi, w których bierze udział nie jedna, a kilka czy nawet kilkanaście osób? Model kolektywnej pracy jest obecnie standardem w naukach ścisłych i biologicznych – w humanistyce i naukach społecznych to wciąż nowość.
Duże zestawy transkrybowanych źródeł wymagają odpowiednich metod analizy. Tu z pomocą historykom przychodzą metody text mining. W dużym skrócie, text mining to koncepcje i metody automatycznej  eksploracji  (czyli analizy) tekstów, zazwyczaj dużych korpusów dokumentów, pozwalające na wy- dobycie z nich określonych, nieujawniających się na pierwszy rzut oka informacji i prawidłowości. Pojęcie to warto odróżnić od pojęcia data mining, które opi- suje metody analizy zestawów danych, posiadających już jednak – w odróżnieniu
od tekstów – ściśle określoną strukturę. Ciekawym przykładem zastosowania w Polsce metod komputerowej analizy tekstów historycznych są badania nad tożsamością  i pisarstwem  Galla  Anonima,  prowadzone  przez  prof. Tomasza Jasińskiego (Biblioteka Kórnicka PAN). Porównywał on za pomocą odpowied- nich statystycznych narzędzi komputerowych styl Kroniki Polskiej oraz utworów przypisywanych temu autorowi, m.in. Historii o translacji św. Mikołaja Wiel- kiego (Jasiński, 2011).
Internet jest bogatym źródłem korpusów tekstów i zestawów danych, które można wykorzystywać w swoich badaniach czy projektach naukowych (bazach danych, wizualizacjach, mashupach, prezentacjach). Wiele instytucji, takich jak np. europejska biblioteka cyfrowa Europeana, udostępnia swoje dane online na zasadach pozwalających każdemu eksplorować je do własnych celów – mó- wimy tu o tzw. open data.  Czasem dostęp do korpusów tekstów bywa płatny
– przykładem może być angielski serwis Electronic Enlightenment, zawierający w pełni transkrybowaną i uzupełnioną hipertekstualnymi odnośnikami korespon- dencję uczestników oświeceniowej republiki uczonych i poetów określaną jako Respublica literaria.  Mediewiści skorzystać mogą z baz, takich jak choćby Patrologia Latina Database, zestawu publikowanych przez firmę ProQuest cy- frowych wydań tekstów Ojców Kościoła, przygotowanego na podstawie pracy XIX-wiecznego wydawcy Jacques'a Paula Migne'a. Baza ta  pozwala na przeglą- danie skanów, ale także przeszukiwanie pełnotekstowe (proste i zaawansowane), korzystanie z aparatu krytycznego (przypisy), dodawanie własnych notatek itp. Wszystkie te możliwości oparte są na w pełni transkrybowanym tekście, uzu- pełnionym o odpowiednie semantyczne znaczniki. Jest to wymagająca żmudnej pracy, ale z pewnością bardziej efektywna forma publikowania tekstów histo- rycznych online, niż udostępnianie jedynie skanów.
Nie wszystkie historyczne bazy danych powstają wyłącznie jako efekty pracy zespołów specjalistów i w ramach dużych budżetów, często także komer- cyjnych podmiotów. Najważniejszym celem takiej pracy nad bazą danych jest przekształcenie ciągu narracyjnego, tj. tekstu, zdań, akapitów, nieregularnych ze- stawów wyrazów, w katalog odpowiednio uszeregowanych danych, co pozwala później na ich szczegółowe przeszukiwanie. W realizowanym przez amerykań- skie archiwa narodowe The U.S. National Archives and Records Administration (NARA) projekcie pod nazwą 1940 Census Community Indexing Project, każdy użytkownik Internetu mógł włączyć się w prace nad przenoszeniem informacji ze skanów kart ze spisu ludności z 1940 roku, udostępnionych online, do bazy danych. Dzięki pomocy ponad 150 tys. wolontariuszy udało się opracować prze- szukiwalny indeks liczący 132 mln nazwisk. Dziś ta wypracowana oddolnie baza danych może być swobodnie wykorzystywana przez historyków, genealogów i wszystkich zainteresowanych historią rodzinną. Więcej o tego typu inicjatywach piszę w jednym z ostatnich rozdziałów podręcznika.
Korzystanie z internetowych historycznych baz danych niesie ze sobą jednak wyzwania dla naukowca, związane z nowymi metodami pracy. Efektywne korzystanie z bazy pełnotekstowej wymaga odpowiednich umiejętności, związanych z formułowaniem zapytań do jej wyszukiwarek i projektowaniem kwerend. Wąt- pliwości wzbudza także kwestia poprawnego informowania o wykorzystanych bazach danych w przypisach do pracy naukowej: jak pisze Krzystof Narojczyk, poprawnie sformułowane odwołanie do tego typu źródła to coś więcej niż stan- dardowe podanie adresu URL (jak w przypadku zwykłych stron WWW). Wyniki wyszukiwania w bazie danych mają charakter dynamiczny, bo zależą przecież od sformułowanych przez użytkownika zapytań (Narojczyk, 2005, s. 38–39).
Nowoczesne teorie wydań cyfrowych źródeł historycznych akcentują ko- nieczność wyjścia poza klasyczny, narracyjny model wykorzystywania tych źró- deł, w ramach którego to badacz samodzielnie analizuje udostępniane treści. Jak pisze Dino Buzzetti (2012), mówiąc o edycjach cyfrowych wciąż myślimy o człowieku, chociaż powinniśmy myśleć raczej o tym, czy i jak komputer może przetwarzać do celów badawczych zawarte w nich informacje. Buzzetti zgadza się ze stwierdzeniem, że naczelną ideą edycji cyfrowych powinno być przeniesie- nie kompetencji czytelnika badacza na oprogramowanie (Buzzetti, 2012, s. 45).
Rosnące zainteresowanie historyków automatycznym przetwarzaniem i ana- lizowaniem dużych korpusów tekstów, obrazów czy zestawów danych wywoduje wiele opinii głoszących moment przejścia historii akademickiej w okres post-teo- retyczny. Dyskusje teoretyczne związane z kolejnymi zwrotami w humanistyce przestają być atrakcyjne, z uwagi na szerokie możliwości badawcze, oferowane przez narzędzia cyfrowe i modele współpracy online. Praktykowanie historii cy- frowej – podobnie jak to było w przypadku historyków działających pod koniec XIX-wieku – to koncentracja na metodach, narzędziach, na organizacji i prze- twarzaniu zbiorów informacji, gdzie zdecydowanie mniejszą uwagę poświęca się ideologiom i wielkim narracjom (Scheinfeldt, 2012, s. 124).
Konieczność wyjścia poza badawcze ideologie i popularne dyskursy opisu- jące przeszłość akcentuje także Lev Manovich i Jeremy Douglas (Manovich, Do- uglas, 2010), nawołując wprost do zejścia na najniższy poziom opisu: do danych (we have to turn „culture” into „data”). Jak pisze, interpretacje w badaniach za- wsze motywowane są określonymi przekonaniami, ideologiami czy modami, w praktyce mamy jednak do czynienia zawsze z materialnymi – i dziś dostępnymi w zdigitalizowanej formie – obiektami kultury, przechowywanymi w muzeach, archiwach, galeriach i bibliotekach. Tworząc narracje o historii kultur i spo- łeczeństw posługujemy się abstrakcyjnymi pojęciami. Według wspomnianych autorów historie te można opisywać równie dobrze za pomocą odpowiednich wizualizacji, opartych na analizowaniu ewolucji podstawowych cech obiektów kultury, tj. kształtów, kolorów, struktury wizualnej obrazów, rzeźb, okładek cza- sopism itp.
Jedną z oponentek takiego programu jest Johanna Drucker. Jej zdaniem ko- nieczny jest ciągły krytyczny namysł nad nowym, cyfrowym kierunkiem badań humanistycznych. Wizualizacje danych historycznych czy automatyczne analizy tekstu mogą być przejawem pozytywistycznego, mechanistycznego, dosłownego i ahistorycznego traktowania zjawisk społecznych i kulturowych. W tego typu badaniach faktycznie odrzuca się podstawowe zasady humanistycznej reflek- sji, które każą podejrzliwie patrzeć na każde pozornie oczywiste fakty i szukać determinujących je czynników, takich jak: historia, polityka, kultura, procesy społeczne, gospodarcze czy choćby gatunkowość, retoryka itp. Johanna Drucker zwraca też uwagę na niebezpieczeństwo przenoszenia na metody badań huma- nistycznych ograniczeń związanych z wykorzystywaną w nich technologią. Jej zdaniem, o ile idea książki została historycznie wykształcona w środowisku hu- manistów, którzy dostosowali to medium do charakteru własnych badań, tak narzędzia cyfrowe, oprogramowanie czy interfejsy są już produktem zupełnie in- nej kultury wiedzy (Drucker, 2012, s. 85–86).
Wróćmy jeszcze na chwilę do potencjału automatycznej eksploracji tekstu w badaniach historycznych. Skoro można automatycznie analizować zasoby pełno- tekstowych archiwów, takich jak London Lives czy Old Bailey Online, dlaczego nie zastosować takiego podejścia do zdecydowanie większych korpusów tek- stów? W opublikowanym w 2011 roku w „Science” artykule Quantitative Ana- lysis of Culture Using Millions of Digitized Books (Michel i in., 2011) autorzy zaproponowali interesujący model ilościowych badań kultury, który określono pojęciem kulturonomii (ang. culturomics). Przygotowane przez Google narzędzie Ngram Viewer pozwala na wizualizowanie na osi czasu częstości występowania wybranych fraz w korpusach treści książek zdigitalizowanych i transkrybowa- nych w ramach programu Google Books. Szacuje się, że w ramach projektu Google Books zeskanowano do tej pory i sczytano za pomocą OCR około 12 proc. wszystkich książek, opublikowanych drukiem od połowy XV wieku (daje to w sumie około 15 milionów tytułów). Na ich bazie Google zbudowało korpus składający się z ponad 500 miliardów wyrazów, w którym – co nie powinno dzi- wić – dominuje głównie język angielski (361 miliardów wyrazów). Są w nim także słowa z języków: francuskiego (45 miliardów), hiszpańskiego (45 miliar- dów), niemieckiego (37 miliardów), chińskiego (13 miliardów), rosyjskiego (35 miliardów) i hebrajskiego (2 miliardy). Chociaż w bazach Google Books można znaleźć książki w języku polskim, narzędzie Ngram Viewer nie pozwala na prze- szukiwanie korpusu języka polskiego. Wszystkie dostępne tam korpusy mają zresztą ograniczoną objętość – twórcy systemu z zasobów 15 milionów dostęp- nych książek wybrali treść 5 milionów. Kryterium wyboru była m.in. jakość OCR i metadanych. Wykluczono także – co ważne! – czasopisma.
Zasada działania Ngram Viewer jest dość prosta – system wizualizuje w czasie częstość występowania w określonych korpusach wybranych przez użyt- kownika słów lub fraz, w odniesieniu do ogólnej liczby wyrazów zindeksowa- nych w zebranych publikacjach z danego roku. Przykładowo, można dzięki temu sprawdzić, jak w anglojęzycznej literaturze funkcjonowało na skali popularności pojęcie niewolnictwo (ang. slavery). Analiza wykresu pozwala zaznaczyć war- tości maksymalne, pokrywające się z ważnymi wydarzeniami historycznymi i społecznymi (np. dla abolicjonizmu są nimi zniesienie niewolnictwa w koloniach Wielkiej Brytanii w 1833 roku czy amerykańska Wojna Domowa).
Czy takie narzędzie może być pomocne w badaniach historycznych? Ngram Viewer  wizualizuje  trendy  kulturowe  jedynie  z  perspektywy  języka,  dzięki czemu można przyglądać się ewolucji popularności określonych pojęć czy po- równywać ze sobą ich rozmaite zestawy. Twórcy systemu przekonują, że odpo- wiednio przemyślane użycie tego narzędzia pozwala np. badać historię cenzury, związaną z występowaniem albo niewystępowaniem określonych pojęć (np. nie- obecność frazy Marc Chagall w literaturze niemieckiej z lat 30. spowodowana była uznaniem kubizmu za sztukę zdegenerowaną). Umożliwia też badanie po- pularności określonych postaci historycznych, historii adaptacji wynalazków, trendów kulturowych, ideologii, np. socjalizmu czy feminizmu.
Brzmi to bardzo atrakcyjnie, jednak warto uświadomić sobie ograniczenia tej metody. Po pierwsze, z perspektywy badaczy zajmujących się historią Polski poważnym mankamentem tego narzędzia jest brak możliwości analizy korpusu książek w języku polskim. Co prawda, wykorzystać można korpus książek ro- syjskich czy niemieckich. Poważniejszy problem wiąże się z faktem, że korpusy analizowane przez Ngram Viewer zbudowane są na bazie jedynie części pu- blikowanych tytułów, z wykluczeniem czasopism czy druków ulotnych. Wadą systemu, bardzo często podkreślaną przez jego krytyków, są podstawowe błędy w automatycznym rozpoznawaniu wyrazów ze skanów, mogące poważnie zabu- rzać jakość wizualizacji (Hayes, 2011, s. 193).
Interesującą krytykę kulturonomii jako metody badań nad kulturą zapropo- nował Tim Hitchcock, twórca wspomnianych już cyfrowych projektów archi- walnych. W opublikowanej na swoim blogu naukowym notce odwołuje się do popularności kliometrii, przypadającej na lata 70. i 80. XX wieku. Zapropono- wany przez twórców Ngram Viewer model badania kultury – w odróżnieniu od tego stosowanego przez wcześniejszych praktyków historycznych badań kwanty- tatywnych – działa bez odniesienia do historycznej wiedzy o dawnych społeczeń- stwach, które nadają odpowiedni kontekst analizom danych liczbowych (Hitch- cock, 2011). Nie da się kultury czy procesu historycznego zredukować jedynie do ciągu danych. Chociaż użycie metod statystycznych w naukach historycznych ma długą tradycję, nie mogą być one przecież uznawane za wystarczające. Z drugiej strony – dostępność danych dotyczących kultury pozwala jednak stawiać nowe pytania badawcze, budować wizualizacje, infografiki, kształtować nowe wizu- alne formy mówienia o przeszłości. Więcej na ten temat znaleźć można w trzeciej części opracowania.
Ważnym problemem, pojawiającym się w kontekście pracy badawczej, jest dostępność danych i ich otwartość. Ngram Viewer to narzędzie zbudowane przez Google, a korpus tekstów, z którego to narzędzie korzysta, jest także własnością Google, chociaż firma ta nie ma praw do poszczególnych treści wchodzących w skład korpusu. Stąd też system wykorzystuje w swoim działaniu nie standardowe słowa kluczowe, ale n-elementowe zestawy wyrazów (ang. n-gram) – których bazy zresztą można pobrać ze strony projektu na jednej z licencji Creative Com-mons – a sam zestaw n-gramów dla korpusu języka angielskiego zajmuje w skompresowanej formie 340 gigabajtów.
Warto zatem podkreślić to, że pojęcie open data związane jest z zagadnie- niem statusu prawnoautorskiego danych (badawczych) i nakłada na naukowca konieczność udostępniania ich na zasadach pozwalających wykorzystywać je swobodnie. Dobrym przykładem takiego podejścia jest Europeana, która agregu- jąc metadane z bibliotek i archiwów europejskich publikuje własne API, pozwa- lające na ich przeszukiwanie i wykorzystywanie (także do celów komercyjnych). Na podobnych zasadach udostępnia swoje zbiory i metadane także amerykańskie Walters Art Museum (http://www.thedigitalwalters.org).

Bibliografia:

Buzzetti, D. (2012). Digital editions and text processing. W: M. Deegan, K. Sutherland (red.),Text Editing Print and the Digital World (s. 45-61). Burlington: Ashgate Publishing.
Drucker, J. (2012). Humanistic theory and digital scholarship. W: M. K. Gold (red.), Debates in the Digital Humanities (s.85-95). Minneapolis: Univer- sity of Minnesota Press. Pobrano z: http://dhdebates.gc.cuny.edu/debates/text/34
Hayes, B. (2011). Bit Lit: With digitized text from five million books, one is never at a loss for words, American Scientist, 99(3), 190–194. Pobrano z                   :                   http://www.americanscientist.org/libraries/documents/
2011412130118486-2011-05CompSciHayes.pdf
Hitchcock, T. (2008). Digital Searching and the Reformulation of Historical Knowledge. W: M. Greengrass, L. Hughes (red.), The Virtual Representation of the Past. Burlington: Ashgate Publishing.
Hitchcock, T. (2011). Culturomics, Big Data, Code Breakers and the Casau- bon Delusion, Historyonics. Pobrano z: http://historyonics.blogspot.com/2011/
06/culturomics-big-data-code-breakers-and.html.
Jasiński, T. (2011). Kronika Polska Galla Anonima w świetle unikatowej analizy komputerowej nowej generacji. Wykłady Inauguracyjne Instytutu Historii Uniwersytetu im. Adama Mickiewicza ; semestr letni 2010/2011, 6, Instytut Hi- storii UAM: Poznań.
Manovich, L., Douglass, J. (2010). Mapping Time: Visualization of Tem- poral Patterns in Media and Art. Pobrano z: http://softwarestudies.com/cultu- ral_analytics/visualizing_temporal_patterns.pdf
Michel, J. B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Pickett, J. P., Hoiberg, D., Aiden, E. L. (2011). Quantitative Analysis of Culture Using Mil- lions of Digitized Books, Science, 331(6014), 176–182.
Narojczyk, K. (2005). Internetowe bazy danych w badaniach historycznych. W: K. Narojczyk, B. Ryszewski (red.), Metody komputerowe w badaniach i na- uczaniu historii. Olsztyn: Wydawnictwo Uniwersytetu Warmińsko-Mazurskiego.
Scheinfeldf, T. (2012). Sunset for Ideology, Sunrise for Methodology? W:
 
Wprowadzenie do historii cyfrowej



Matthew K. Gold (red.), Debates in the Digital Humanities (s.124-126). Minne- apolis: University of Minnesota Press. Pobrano z: http://dhdebates.gc.cuny.edu/ debates/text/39.
